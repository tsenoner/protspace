{
  "cells": [
    {
      "cell_type": "markdown",
      "id": "0053037c",
      "metadata": {},
      "source": [
        "# Protein Language Model Embeddings Generator"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "de97cfd4",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title 1. Install Required Dependencies\n",
        "# @markdown Run this cell first to install the necessary packages\n",
        "%%capture\n",
        "!pip install h5py numpy pandas pyfaidx torch tqdm transformers esm huggingface_hub"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "32dcabb3",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### 2. Import Libraries and Setup\n",
        "from pathlib import Path\n",
        "import h5py\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "import torch\n",
        "from pyfaidx import Fasta\n",
        "from tqdm import tqdm\n",
        "from transformers import AutoTokenizer, EsmModel, T5EncoderModel, T5Tokenizer\n",
        "from google.colab import drive, files, userdata\n",
        "\n",
        "# Imports for Native ESM and HF Login\n",
        "from esm.models.esm3 import ESM3\n",
        "from esm.models.esmc import ESMC\n",
        "from esm.sdk.api import ESMProtein, SamplingConfig, LogitsConfig\n",
        "from huggingface_hub import login as hf_login, HfFolder"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "55eff302",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### Optional: Hugging Face Login (Needed for models like ESM3, ESMC)\n",
        "# @markdown If you're using a model that requires authentication (e.g., native ESM models from EvolutionaryScale),\n",
        "# @markdown run this cell and enter your Hugging Face token when prompted.\n",
        "# @markdown You can get a token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
        "# @markdown Leave blank and run if you have already configured login in this Colab environment or the model is public.\n",
        "hf_token_input = \"\"  # @param {type:\"string\"}\n",
        "\n",
        "if hf_token_input:\n",
        "    hf_login(token=hf_token_input)\n",
        "    print(\"Attempted login with provided token.\")\n",
        "elif userdata.get(\"HF_TOKEN\"):\n",
        "    print(\"Attempted login with secret 'HF_TOKEN' token\")\n",
        "elif HfFolder.get_token() is not None:\n",
        "    print(\"Already logged in to Hugging Face Hub (found existing token/credentials).\")\n",
        "else:\n",
        "    print(\"Not logged in. Native ESM models may fail if they require authentication.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "8052fc1e",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title Optional: Mount Google Drive\n",
        "# @markdown If you want to use a FASTA file from your Google Drive, run this cell to mount your drive.\n",
        "# @markdown It will prompt for authorization the first time.\n",
        "should_mount_drive = False  # @param {type:\"boolean\"}\n",
        "if should_mount_drive:\n",
        "    try:\n",
        "        drive.mount(\"/content/drive\")\n",
        "        print(\"Google Drive mounted successfully.\")\n",
        "    except Exception as e:\n",
        "        print(f\"Error mounting Google Drive: {e}\")\n",
        "else:\n",
        "    print(\"Skipping Google Drive mount.\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0077526c",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### 3. Select Model and Upload File\n",
        "\n",
        "# @markdown Choose a protein language model:\n",
        "model_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # @param [\"Rostlab/prot_t5_xl_half_uniref50-enc\", \"Rostlab/ProstT5_fp16\", \"ElnaggarLab/ankh-base\", \"ElnaggarLab/ankh-large\", \"facebook/esm2_t6_8M_UR50D\", \"facebook/esm2_t12_35M_UR50D\", \"facebook/esm2_t30_150M_UR50D\", \"facebook/esm2_t33_650M_UR50D\", \"EvolutionaryScale/esm3-sm-open-v1\", \"EvolutionaryScale/esmc-300m-2024-12\", \"EvolutionaryScale/esmc-600m-2024-12\"]\n",
        "\n",
        "# @markdown Choose embedding type:\n",
        "embedding_type = \"per_prot\"  # @param [\"per_prot\", \"per_res\"]\n",
        "\n",
        "# @markdown Set maximum sequence length (longer sequences will be skipped):\n",
        "max_sequence_length = 2000  # @param {type:\"integer\"}\n",
        "\n",
        "# @markdown Enter a Google Drive path or upload from your computer:\n",
        "fasta_filename = \"\"  # @param {type:\"string\", placeholder:\"Path to FASTA file in Google Drive (leave empty to upload)\"}\n",
        "if not fasta_filename:\n",
        "    uploaded = files.upload()\n",
        "    fasta_filename = list(uploaded.keys())[0]\n",
        "\n",
        "MODEL_NAME_TO_SHORT_KEY_MAP = {\n",
        "    \"Rostlab/prot_t5_xl_half_uniref50-enc\": \"prot_t5\",\n",
        "    \"Rostlab/ProstT5_fp16\": \"prost_t5\",\n",
        "    \"ElnaggarLab/ankh-base\": \"ankh_base\",\n",
        "    \"ElnaggarLab/ankh-large\": \"ankh_large\",\n",
        "    \"facebook/esm2_t6_8M_UR50D\": \"esm2_8m\",\n",
        "    \"facebook/esm2_t12_35M_UR50D\": \"esm2_35m\",\n",
        "    \"facebook/esm2_t30_150M_UR50D\": \"esm2_150m\",\n",
        "    \"facebook/esm2_t33_650M_UR50D\": \"esm2_650m\",\n",
        "    \"EvolutionaryScale/esm3-sm-open-v1\": \"esm3_open\",\n",
        "    \"EvolutionaryScale/esmc-300m-2024-12\": \"esmc_300m\",\n",
        "    \"EvolutionaryScale/esmc-600m-2024-12\": \"esmc_600m\",\n",
        "}\n",
        "\n",
        "\n",
        "def seq_preprocess(df, model_type=\"esm\"):\n",
        "    # Store raw sequence before preprocessing\n",
        "    df[\"raw_sequence\"] = df[\"sequence\"]\n",
        "\n",
        "    # Replace special amino acids with X\n",
        "    df[\"sequence\"] = df[\"sequence\"].str.replace(\"[BJOUZ]\", \"X\", regex=True)\n",
        "\n",
        "    # Add spaces between amino acids for specific models\n",
        "    if model_type in [\"prost_t5\", \"prot_t5\"]:\n",
        "        df[\"sequence\"] = df[\"sequence\"].apply(lambda seq: \" \".join(list(seq)))\n",
        "\n",
        "    # Add prefix for ProstT5 model\n",
        "    if model_type == \"prost_t5\":\n",
        "        df[\"sequence\"] = df[\"sequence\"].apply(lambda seq: \"<AA2fold> \" + seq)\n",
        "\n",
        "    return df\n",
        "\n",
        "\n",
        "# @markdown ---\n",
        "def setup_model(checkpoint):\n",
        "    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
        "    model = None\n",
        "    tokenizer = None\n",
        "    mod_type = \"unknown\"\n",
        "\n",
        "    print(f\"Setting up model: {checkpoint} on {device}\")\n",
        "\n",
        "    if \"esm2\" in checkpoint:\n",
        "        mod_type = \"esm_transformer\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        model = EsmModel.from_pretrained(checkpoint)\n",
        "    elif \"esm3-sm-open-v1\" in checkpoint:\n",
        "        mod_type = \"native_esm3\"\n",
        "        model = ESM3.from_pretrained(\"esm3-open\")\n",
        "    elif checkpoint == \"EvolutionaryScale/esmc-300m-2024-12\":\n",
        "        mod_type = \"native_esmc\"\n",
        "        model = ESMC.from_pretrained(\"esmc_300m\")\n",
        "    elif checkpoint == \"EvolutionaryScale/esmc-600m-2024-12\":\n",
        "        mod_type = \"native_esmc\"\n",
        "        model = ESMC.from_pretrained(\"esmc_600m\")\n",
        "    elif \"ankh\" in checkpoint:\n",
        "        mod_type = \"ankh\"\n",
        "        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n",
        "        model = T5EncoderModel.from_pretrained(checkpoint)\n",
        "    elif \"prot_t5\" in checkpoint or \"ProstT5\" in checkpoint:\n",
        "        is_prostt5 = \"prostt5\" in checkpoint.lower()\n",
        "        mod_type = \"prost_t5\" if is_prostt5 else \"prot_t5\"\n",
        "        tokenizer = T5Tokenizer.from_pretrained(\n",
        "            checkpoint, do_lower_case=not is_prostt5\n",
        "        )\n",
        "        model = T5EncoderModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n",
        "        if device.type == \"cuda\":\n",
        "            model = model.half()\n",
        "    else:\n",
        "        raise ValueError(f\"Unknown model checkpoint type: {checkpoint}\")\n",
        "\n",
        "    return model.to(device), tokenizer, mod_type\n",
        "\n",
        "\n",
        "def read_fasta(file_path):\n",
        "    headers = []\n",
        "    sequences = []\n",
        "    with Fasta(str(file_path)) as fasta_data:\n",
        "        for seq_record in fasta_data:\n",
        "            headers.append(seq_record.name)\n",
        "            sequences.append(str(seq_record))\n",
        "    return headers, sequences\n",
        "\n",
        "\n",
        "def create_embedding(\n",
        "    checkpoint,\n",
        "    df,\n",
        "    emb_type=\"per_prot\",\n",
        "    output_file=\"protein_embeddings.h5\",\n",
        "    max_len=2000,\n",
        "):\n",
        "    print(\"Setting up model...\")\n",
        "    model_instance, tokenizer_instance, mod_type = setup_model(checkpoint)\n",
        "    model_instance.eval()\n",
        "    df_processed = seq_preprocess(df.copy(), mod_type)\n",
        "    device = model_instance.device\n",
        "\n",
        "    def compute_embedding(\n",
        "        sequence, current_emb_type, current_mod_type, model, tokenizer\n",
        "    ):\n",
        "        if current_mod_type.startswith(\"native_esm\"):\n",
        "            protein = ESMProtein(sequence=sequence)\n",
        "            tokens = model.encode(protein).to(device)\n",
        "            raw_per_residue_embeddings = None\n",
        "            if current_mod_type == \"native_esm3\":\n",
        "                out = model.forward_and_sample(\n",
        "                    tokens, SamplingConfig(return_per_residue_embeddings=True)\n",
        "                )\n",
        "                raw_per_residue_embeddings = (\n",
        "                    out.per_residue_embedding.squeeze(0).cpu().numpy()\n",
        "                )\n",
        "            elif current_mod_type == \"native_esmc\":\n",
        "                out = model.logits(\n",
        "                    tokens, LogitsConfig(sequence=True, return_embeddings=True)\n",
        "                )\n",
        "                raw_per_residue_embeddings = out.embeddings.squeeze(0).cpu().numpy()\n",
        "            else:\n",
        "                raise ValueError(f\"Unknown native ESM model type: {current_mod_type}\")\n",
        "\n",
        "            # Directly slice to remove BOS/EOS tokens\n",
        "            cleaned_per_residue_embeddings = raw_per_residue_embeddings[1:-1, :]\n",
        "\n",
        "            if current_emb_type == \"per_prot\":\n",
        "                return cleaned_per_residue_embeddings.mean(axis=0)\n",
        "            elif current_emb_type == \"per_res\":\n",
        "                return cleaned_per_residue_embeddings\n",
        "        else:  # Transformers-based models\n",
        "            if tokenizer is None:\n",
        "                raise ValueError(\n",
        "                    f\"Tokenizer not available for model type: {current_mod_type}\"\n",
        "                )\n",
        "\n",
        "            tokenization_input = sequence\n",
        "            tokenizer_call_kwargs = {\n",
        "                \"return_tensors\": \"pt\",\n",
        "                \"max_length\": max_len + 2,\n",
        "                \"truncation\": True,\n",
        "                \"padding\": True,\n",
        "                \"add_special_tokens\": True,  # Adds EOS for T5 models\n",
        "            }\n",
        "\n",
        "            inputs = tokenizer(tokenization_input, **tokenizer_call_kwargs).to(device)\n",
        "\n",
        "            with torch.no_grad():\n",
        "                embeddings_tensor = model(**inputs).last_hidden_state.cpu()\n",
        "            if embeddings_tensor.ndim == 3 and embeddings_tensor.shape[0] == 1:\n",
        "                embeddings_tensor = embeddings_tensor.squeeze(0)\n",
        "            embeddings_np_arr = embeddings_tensor.numpy()\n",
        "\n",
        "            # Slicing logic based on model type\n",
        "            token_embeddings_for_avg = None\n",
        "            per_residue_slice = None\n",
        "\n",
        "            if current_mod_type == \"esm_transformer\":\n",
        "                # Skip <cls> and <eos>\n",
        "                token_embeddings_for_avg = embeddings_np_arr[1:-1, :]\n",
        "                per_residue_slice = embeddings_np_arr[1:-1, :]\n",
        "            elif current_mod_type == \"prost_t5\":\n",
        "                # Skip <AA2fold> (and potential BOS) and <eos>\n",
        "                token_embeddings_for_avg = embeddings_np_arr[1:-1, :]\n",
        "                per_residue_slice = embeddings_np_arr[1:-1, :]\n",
        "            elif current_mod_type in [\"prot_t5\", \"ankh\"]:  # Regular ProtT5 and Ankh\n",
        "                # Skip <eos>\n",
        "                token_embeddings_for_avg = embeddings_np_arr[:-1, :]\n",
        "                per_residue_slice = embeddings_np_arr[:-1, :]\n",
        "            else:  # Should not happen if mod_type is correctly set\n",
        "                token_embeddings_for_avg = embeddings_np_arr\n",
        "                per_residue_slice = embeddings_np_arr\n",
        "\n",
        "            if current_emb_type == \"per_prot\":\n",
        "                if token_embeddings_for_avg.shape[0] == 0:\n",
        "                    print(\n",
        "                        f\"Warning: No token embeddings to average for sequence after slicing for model {current_mod_type}. Original shape: {embeddings_np_arr.shape}\"\n",
        "                    )\n",
        "                    return np.array([])  # Or handle as error appropriately\n",
        "                return token_embeddings_for_avg.mean(axis=0)\n",
        "            elif current_emb_type == \"per_res\":\n",
        "                return per_residue_slice\n",
        "\n",
        "        raise ValueError(\n",
        "            f\"Invalid embedding type '{current_emb_type}' or model type '{current_mod_type}' combination if not returned by above\"\n",
        "        )\n",
        "\n",
        "    invalid_headers = df_processed[df_processed[\"header\"].str.contains(\"/\")][\n",
        "        \"header\"\n",
        "    ].tolist()\n",
        "    if invalid_headers:\n",
        "        error_msg = (\n",
        "            \"ERROR: The following sequence headers contain '/' which is not allowed in HDF5 dataset names (HDF5 prior to groups):\\n\"\n",
        "            + \"\\n\".join(invalid_headers)\n",
        "            + \"\\nPlease remove or replace '/' characters in these headers and try again.\"\n",
        "        )\n",
        "        raise ValueError(error_msg)\n",
        "\n",
        "    print(\"Generating embeddings...\")\n",
        "    with h5py.File(output_file, \"a\") as hdf:\n",
        "        for _, row_data in tqdm(df_processed.iterrows(), total=len(df_processed)):\n",
        "            sequence_val = row_data[\"sequence\"]\n",
        "            header_val = row_data[\"header\"]\n",
        "\n",
        "            # Skip sequences that exceed the maximum length\n",
        "            if len(row_data[\"raw_sequence\"]) > max_len:\n",
        "                tqdm.write(\n",
        "                    f\"Skipping sequence {header_val} (length {len(row_data['raw_sequence'])}) as it exceeds max length {max_len}.\"\n",
        "                )\n",
        "                continue\n",
        "\n",
        "            if header_val in hdf:\n",
        "                tqdm.write(f\"Skipping existing embedding for {header_val}\")\n",
        "                continue\n",
        "            embedding_result = compute_embedding(\n",
        "                sequence_val, emb_type, mod_type, model_instance, tokenizer_instance\n",
        "            )\n",
        "            hdf.create_dataset(\n",
        "                name=header_val,\n",
        "                data=embedding_result.astype(np.float32)\n",
        "                if isinstance(embedding_result, np.ndarray)\n",
        "                else embedding_result,\n",
        "            )\n",
        "\n",
        "    del model_instance, tokenizer_instance\n",
        "    if torch.cuda.is_available():\n",
        "        torch.cuda.empty_cache()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "3c7e9e21",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### 4. Generate Embeddings\n",
        "# @markdown Click the play button to start generating embeddings\n",
        "\n",
        "fasta_path = Path(fasta_filename)\n",
        "\n",
        "# Get the short key for the selected model_name, fallback to sanitized full name\n",
        "short_model_key = MODEL_NAME_TO_SHORT_KEY_MAP.get(\n",
        "    model_name, model_name.replace(\"/\", \"_\")\n",
        ")\n",
        "output_file = str(fasta_path.with_name(f\"{fasta_path.stem}_{short_model_key}.h5\"))\n",
        "\n",
        "headers, sequences = read_fasta(fasta_path)\n",
        "df = pd.DataFrame({\"header\": headers, \"sequence\": sequences})\n",
        "print(f\"Processing {len(df)} sequences for model {model_name}...\")\n",
        "\n",
        "create_embedding(\n",
        "    model_name,\n",
        "    df,\n",
        "    emb_type=embedding_type,\n",
        "    output_file=output_file,\n",
        "    max_len=max_sequence_length,\n",
        ")\n",
        "\n",
        "print(f\"\\nEmbeddings saved to {output_file}\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "id": "0eb9a4b1",
      "metadata": {
        "cellView": "form"
      },
      "outputs": [],
      "source": [
        "# @title ### 5. Download Results\n",
        "# @markdown Run this cell to download your embeddings file\n",
        "files.download(output_file)"
      ]
    }
  ],
  "metadata": {
    "jupytext": {
      "cell_metadata_filter": "+cellView",
      "cell_metadata_json": true,
      "main_language": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}
