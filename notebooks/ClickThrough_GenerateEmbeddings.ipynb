{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "0053037c",
   "metadata": {
    "id": "0053037c"
   },
   "source": [
    "# Protein Language Model Embeddings Generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de97cfd4",
   "metadata": {
    "cellView": "form",
    "id": "de97cfd4"
   },
   "outputs": [],
   "source": [
    "# @title 1. Install Required Dependencies\n",
    "# @markdown Run this cell first to install the necessary packages\n",
    "%%capture\n",
    "!pip install h5py numpy pandas pyfaidx torch tqdm transformers esm huggingface_hub"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "32dcabb3",
   "metadata": {
    "cellView": "form",
    "id": "32dcabb3"
   },
   "outputs": [],
   "source": [
    "# @title ### 2. Import Libraries and Setup\n",
    "import re\n",
    "from pathlib import Path\n",
    "\n",
    "import h5py\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import torch\n",
    "from esm.models.esm3 import ESM3\n",
    "from esm.models.esmc import ESMC\n",
    "from esm.sdk.api import ESMProtein, LogitsConfig, SamplingConfig\n",
    "from google.colab import drive, files, userdata\n",
    "from huggingface_hub import HfFolder\n",
    "from huggingface_hub import login as hf_login\n",
    "from pyfaidx import Fasta\n",
    "from tqdm.auto import tqdm\n",
    "from transformers import AutoTokenizer, EsmModel, T5EncoderModel, T5Tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55eff302",
   "metadata": {
    "cellView": "form",
    "id": "55eff302"
   },
   "outputs": [],
   "source": [
    "# @title ### Optional: Hugging Face Login (Needed for models like ESM3, ESMC)\n",
    "# @markdown If you're using a model that requires authentication (e.g., native ESM models from EvolutionaryScale),\n",
    "# @markdown run this cell and enter your Hugging Face token when prompted.\n",
    "# @markdown You can get a token from [https://huggingface.co/settings/tokens](https://huggingface.co/settings/tokens).\n",
    "# @markdown Leave blank and run if you have already configured login in this Colab environment or the model is public.\n",
    "hf_token_input = \"\"  # @param {type:\"string\"}\n",
    "\n",
    "try:\n",
    "    hf_token_secret = userdata.get(\"HF_TOKEN\")\n",
    "except Exception:\n",
    "    hf_token_secret = None\n",
    "\n",
    "if hf_token_input:\n",
    "    hf_login(token=hf_token_input)\n",
    "    print(\"Logged in with provided token.\")\n",
    "elif hf_token_secret:\n",
    "    hf_login(token=hf_token_secret)\n",
    "    print(\"Logged in with Colab secret 'HF_TOKEN'.\")\n",
    "elif HfFolder.get_token() is not None:\n",
    "    print(\"Already logged in to Hugging Face Hub (found existing token/credentials).\")\n",
    "else:\n",
    "    print(\"Not logged in. Native ESM models may fail if they require authentication.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8052fc1e",
   "metadata": {
    "cellView": "form",
    "id": "8052fc1e"
   },
   "outputs": [],
   "source": [
    "# @title Optional: Mount Google Drive\n",
    "# @markdown If you want to use a FASTA file from your Google Drive, run this cell to mount your drive.\n",
    "# @markdown It will prompt for authorization the first time.\n",
    "should_mount_drive = False  # @param {type:\"boolean\"}\n",
    "if should_mount_drive:\n",
    "    try:\n",
    "        drive.mount(\"/content/drive\")\n",
    "        print(\"Google Drive mounted successfully.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error mounting Google Drive: {e}\")\n",
    "else:\n",
    "    print(\"Skipping Google Drive mount.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0077526c",
   "metadata": {
    "cellView": "form",
    "id": "0077526c"
   },
   "outputs": [],
   "source": "# @title ### 3. Select Model and Upload File\n\n# @markdown Choose a protein language model:\nmodel_name = \"Rostlab/prot_t5_xl_half_uniref50-enc\"  # @param [\"Rostlab/prot_t5_xl_half_uniref50-enc\", \"Rostlab/ProstT5_fp16\", \"ElnaggarLab/ankh-base\", \"ElnaggarLab/ankh-large\", \"facebook/esm2_t6_8M_UR50D\", \"facebook/esm2_t12_35M_UR50D\", \"facebook/esm2_t30_150M_UR50D\", \"facebook/esm2_t33_650M_UR50D\", \"EvolutionaryScale/esm3-sm-open-v1\", \"EvolutionaryScale/esmc-300m-2024-12\", \"EvolutionaryScale/esmc-600m-2024-12\"]\n\n# @markdown Choose embedding type:\nembedding_type = \"per_prot\"  # @param [\"per_prot\", \"per_res\"]\n\n# @markdown Set maximum sequence length (longer sequences will be skipped):\nmax_sequence_length = 2000  # @param {type:\"integer\"}\n\n# @markdown Max batch size for transformer models (auto-reduces on GPU OOM):\nbatch_size = 128  # @param {type:\"integer\"}\n\n# @markdown Enter a Google Drive path or upload from your computer:\nfasta_filename = \"\"  # @param {type:\"string\", placeholder:\"Path to FASTA file in Google Drive (leave empty to upload)\"}\nif not fasta_filename:\n    uploaded = files.upload()\n    fasta_filename = list(uploaded.keys())[0]"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fw0fqzkoi6",
   "metadata": {},
   "outputs": [],
   "source": "# @title ### Functions { display-mode: \"form\" }\n\nMODEL_SHORT_KEYS = {\n    \"Rostlab/prot_t5_xl_half_uniref50-enc\": \"prot_t5\",\n    \"Rostlab/ProstT5_fp16\": \"prost_t5\",\n    \"ElnaggarLab/ankh-base\": \"ankh_base\",\n    \"ElnaggarLab/ankh-large\": \"ankh_large\",\n    \"facebook/esm2_t6_8M_UR50D\": \"esm2_8m\",\n    \"facebook/esm2_t12_35M_UR50D\": \"esm2_35m\",\n    \"facebook/esm2_t30_150M_UR50D\": \"esm2_150m\",\n    \"facebook/esm2_t33_650M_UR50D\": \"esm2_650m\",\n    \"EvolutionaryScale/esm3-sm-open-v1\": \"esm3_open\",\n    \"EvolutionaryScale/esmc-300m-2024-12\": \"esmc_300m\",\n    \"EvolutionaryScale/esmc-600m-2024-12\": \"esmc_600m\",\n}\n\n\ndef preprocess_sequences(df, model_type):\n    \"\"\"Prepare sequences for the given model type. Returns a new DataFrame.\"\"\"\n    df = df.copy()\n    df[\"raw_sequence\"] = df[\"sequence\"]\n    df[\"sequence\"] = df[\"sequence\"].str.replace(\"[BJOUZ]\", \"X\", regex=True)\n    if model_type in (\"prost_t5\", \"prot_t5\"):\n        df[\"sequence\"] = df[\"sequence\"].apply(lambda s: \" \".join(s))\n    if model_type == \"prost_t5\":\n        df[\"sequence\"] = \"<AA2fold> \" + df[\"sequence\"]\n    return df\n\n\ndef setup_model(checkpoint):\n    \"\"\"Load model and tokenizer for the given checkpoint.\"\"\"\n    device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n    print(f\"Setting up model: {checkpoint} on {device}\")\n\n    if \"esm2\" in checkpoint:\n        mod_type = \"esm_transformer\"\n        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        model = EsmModel.from_pretrained(checkpoint)\n    elif \"esm3-sm-open-v1\" in checkpoint:\n        mod_type = \"native_esm3\"\n        tokenizer = None\n        model = ESM3.from_pretrained(\"esm3-open\")\n    elif checkpoint == \"EvolutionaryScale/esmc-300m-2024-12\":\n        mod_type = \"native_esmc\"\n        tokenizer = None\n        model = ESMC.from_pretrained(\"esmc_300m\")\n    elif checkpoint == \"EvolutionaryScale/esmc-600m-2024-12\":\n        mod_type = \"native_esmc\"\n        tokenizer = None\n        model = ESMC.from_pretrained(\"esmc_600m\")\n    elif \"ankh\" in checkpoint:\n        mod_type = \"ankh\"\n        tokenizer = AutoTokenizer.from_pretrained(checkpoint)\n        model = T5EncoderModel.from_pretrained(checkpoint)\n    elif \"prot_t5\" in checkpoint or \"ProstT5\" in checkpoint:\n        is_prostt5 = \"prostt5\" in checkpoint.lower()\n        mod_type = \"prost_t5\" if is_prostt5 else \"prot_t5\"\n        tokenizer = T5Tokenizer.from_pretrained(\n            checkpoint, do_lower_case=not is_prostt5\n        )\n        model = T5EncoderModel.from_pretrained(checkpoint, torch_dtype=torch.float16)\n        if device.type == \"cuda\":\n            model = model.half()\n    else:\n        raise ValueError(f\"Unknown model checkpoint: {checkpoint}\")\n\n    return model.to(device), tokenizer, mod_type\n\n\ndef read_fasta(file_path):\n    \"\"\"Read a FASTA file and return headers and sequences.\"\"\"\n    headers, sequences = [], []\n    with Fasta(str(file_path)) as fasta_data:\n        for record in fasta_data:\n            headers.append(record.name)\n            sequences.append(str(record))\n    return headers, sequences\n\n\ndef _embed_native_esm(sequence, emb_type, mod_type, model, device):\n    \"\"\"Compute embeddings using native ESM3/ESMC models.\"\"\"\n    protein = ESMProtein(sequence=sequence)\n    tokens = model.encode(protein).to(device)\n\n    with torch.inference_mode():\n        if mod_type == \"native_esm3\":\n            out = model.forward_and_sample(\n                tokens, SamplingConfig(return_per_residue_embeddings=True)\n            )\n            per_res = out.per_residue_embedding.squeeze(0).cpu().float().numpy()\n        else:  # native_esmc\n            out = model.logits(\n                tokens, LogitsConfig(sequence=True, return_embeddings=True)\n            )\n            per_res = out.embeddings.squeeze(0).cpu().float().numpy()\n\n    per_res = per_res[1:-1, :]  # strip BOS/EOS\n    return per_res.mean(axis=0) if emb_type == \"per_prot\" else per_res\n\n\ndef _embed_transformers_batch(sequences, emb_type, mod_type, model, tokenizer, max_len):\n    \"\"\"Compute embeddings for a batch using HuggingFace Transformers.\"\"\"\n    inputs = tokenizer(\n        sequences,\n        return_tensors=\"pt\",\n        max_length=max_len + 2,\n        truncation=True,\n        padding=True,\n        add_special_tokens=True,\n    ).to(model.device)\n\n    with torch.inference_mode():\n        hidden = model(**inputs).last_hidden_state.cpu().float().numpy()\n\n    lengths = inputs[\"attention_mask\"].sum(dim=1).tolist()\n\n    results = []\n    for i, seq_len in enumerate(lengths):\n        # Strip special tokens: ESM2/ProstT5 have <cls>/<AA2fold> and <eos>,\n        # ProtT5/Ankh have only <eos>\n        if mod_type in (\"esm_transformer\", \"prost_t5\"):\n            emb = hidden[i, 1 : seq_len - 1, :]\n        else:  # prot_t5, ankh\n            emb = hidden[i, : seq_len - 1, :]\n        results.append(emb.mean(axis=0) if emb_type == \"per_prot\" else emb)\n\n    return results\n\n\ndef _write_batch(hdf, headers, embeddings):\n    \"\"\"Write a batch of embeddings to an open HDF5 file.\"\"\"\n    for header, emb in zip(headers, embeddings, strict=False):\n        hdf.create_dataset(name=header, data=emb.astype(np.float32))\n\n\ndef create_embedding(\n    checkpoint,\n    df,\n    emb_type=\"per_prot\",\n    output_file=\"protein_embeddings.h5\",\n    max_len=2000,\n    batch_size=32,\n):\n    \"\"\"Generate embeddings and write them to an HDF5 file.\"\"\"\n    print(\"Setting up model...\")\n    model, tokenizer, mod_type = setup_model(checkpoint)\n    model.eval()\n    df_proc = preprocess_sequences(df, mod_type)\n\n    # Validate headers (HDF5 doesn't allow '/' in dataset names)\n    bad = df_proc[df_proc[\"header\"].str.contains(\"/\")][\"header\"].tolist()\n    if bad:\n        raise ValueError(\n            \"Headers contain '/' (invalid for HDF5):\\n\"\n            + \"\\n\".join(bad)\n            + \"\\nPlease fix these headers and try again.\"\n        )\n\n    with h5py.File(output_file, \"a\") as hdf:\n        # Skip already-computed embeddings via set comparison\n        existing = set(hdf.keys())\n        already_done = df_proc[\"header\"].isin(existing)\n        too_long = df_proc[\"raw_sequence\"].str.len() > max_len\n        to_compute = df_proc[~already_done & ~too_long].copy()\n\n        n_existing = already_done.sum()\n        n_too_long = (~already_done & too_long).sum()\n        if n_existing:\n            print(f\"Skipping {n_existing} already computed embeddings\")\n        if n_too_long:\n            print(f\"Skipping {n_too_long} sequences exceeding max length {max_len}\")\n\n        if to_compute.empty:\n            print(\"All embeddings already computed!\")\n            return\n\n        # Sort by sequence length for efficient batching (shortest first)\n        to_compute = to_compute.sort_values(\n            by=\"raw_sequence\", key=lambda s: s.str.len()\n        )\n\n        is_native = mod_type.startswith(\"native_esm\")\n        device = model.device\n        print(f\"Computing {len(to_compute)} embeddings...\")\n\n        if is_native:\n            # Native ESM: single sequence at a time (API limitation)\n            for _, row in tqdm(to_compute.iterrows(), total=len(to_compute)):\n                emb = _embed_native_esm(\n                    row[\"sequence\"], emb_type, mod_type, model, device\n                )\n                hdf.create_dataset(name=row[\"header\"], data=emb.astype(np.float32))\n        else:\n            # Transformers: batched with adaptive OOM recovery.\n            # Sequences are sorted shortest-first, so once we reduce the\n            # batch size we never need to increase it again.\n            pbar = tqdm(total=len(to_compute))\n            i = 0\n            bs = batch_size\n            while i < len(to_compute):\n                batch = to_compute.iloc[i : i + bs]\n                try:\n                    embeddings = _embed_transformers_batch(\n                        batch[\"sequence\"].tolist(),\n                        emb_type,\n                        mod_type,\n                        model,\n                        tokenizer,\n                        max_len,\n                    )\n                    _write_batch(hdf, batch[\"header\"], embeddings)\n                    pbar.update(len(batch))\n                    i += bs\n                except torch.cuda.OutOfMemoryError:\n                    torch.cuda.empty_cache()\n                    if bs > 1:\n                        bs = max(1, bs // 2)\n                        tqdm.write(f\"GPU OOM â€” reducing batch size to {bs}\")\n                    else:\n                        tqdm.write(\n                            f\"Skipping {batch.iloc[0]['header']} \"\n                            f\"(len={len(batch.iloc[0]['raw_sequence'])}, OOM at batch_size=1)\"\n                        )\n                        i += 1\n                        pbar.update(1)\n            pbar.close()\n\n    del model, tokenizer\n    if torch.cuda.is_available():\n        torch.cuda.empty_cache()"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3c7e9e21",
   "metadata": {
    "cellView": "form",
    "id": "3c7e9e21"
   },
   "outputs": [],
   "source": [
    "# @title ### 4. Generate Embeddings\n",
    "# @markdown Click the play button to start generating embeddings\n",
    "\n",
    "# @markdown Extract UniProt accession ID from FASTA header?\n",
    "# @markdown If unchecked, the full FASTA header will be used as the identifier.\n",
    "should_extract_uniprot_id = False  # @param {type:\"boolean\"}\n",
    "\n",
    "fasta_path = Path(fasta_filename)\n",
    "\n",
    "# Get the short key for the selected model_name, fallback to sanitized full name\n",
    "short_model_key = MODEL_SHORT_KEYS.get(model_name, model_name.replace(\"/\", \"_\"))\n",
    "output_file = str(fasta_path.with_name(f\"{fasta_path.stem}_{short_model_key}.h5\"))\n",
    "\n",
    "headers, sequences = read_fasta(fasta_path)\n",
    "df = pd.DataFrame({\"header\": headers, \"sequence\": sequences})\n",
    "print(f\"Processing {len(df)} sequences for model {model_name}...\")\n",
    "\n",
    "# Regex to extract UniProt ID\n",
    "uniprot_regex = r\"[OPQ][0-9][A-Z0-9]{3}[0-9]|[A-NR-Z][0-9]([A-Z][A-Z0-9]{2}[0-9]){1,2}\"\n",
    "\n",
    "if should_extract_uniprot_id:\n",
    "    df[\"header\"] = df[\"header\"].apply(\n",
    "        lambda header: re.search(uniprot_regex, header).group(0)\n",
    "        if re.search(uniprot_regex, header)\n",
    "        else header\n",
    "    )\n",
    "else:\n",
    "    print(\"Skipping UniProt ID extraction. Using full FASTA headers.\")\n",
    "\n",
    "create_embedding(\n",
    "    model_name,\n",
    "    df,\n",
    "    emb_type=embedding_type,\n",
    "    output_file=output_file,\n",
    "    max_len=max_sequence_length,\n",
    "    batch_size=batch_size,\n",
    ")\n",
    "\n",
    "print(f\"\\nEmbeddings saved to {output_file}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0eb9a4b1",
   "metadata": {
    "cellView": "form",
    "id": "0eb9a4b1"
   },
   "outputs": [],
   "source": [
    "# @title ### 5. Download Results\n",
    "# @markdown Run this cell to download your embeddings file\n",
    "files.download(output_file)"
   ]
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "gpuType": "T4",
   "provenance": []
  },
  "jupytext": {
   "cell_metadata_filter": "+cellView",
   "cell_metadata_json": true,
   "main_language": "python"
  },
  "kernelspec": {
   "display_name": "Python 3",
   "name": "python3"
  },
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}